# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import table
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# 2. íŒŒì¼ ì—…ë¡œë“œ ë° ë°ì´í„° ë¡œë“œ
from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['heart.csv']))  # ì—…ë¡œë“œí•œ heart.csv

X = df.drop('target', axis=1)
y = df['target']
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 3. ëª¨ë“  ëª¨ë¸ê³¼ íŒŒë¼ë¯¸í„° ì„¤ì • í›„ ê²°ê³¼ ì €ì¥
def cross_validate(model, label, model_name):
    scores = []
    for train_idx, test_idx in kf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        scores.append({
            'Model': model_name,
            'Param': label,
            'Accuracy': accuracy_score(y_test, y_pred),
            'Precision': precision_score(y_test, y_pred),
            'Recall': recall_score(y_test, y_pred),
            'F1 Score': f1_score(y_test, y_pred)
        })
    return scores

# 4. ê°ê° ëª¨ë¸ë³„ë¡œ íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í–‰
results = []

# Logistic Regression
for C in [0.01, 0.1, 1, 10, 100]:
    model = LogisticRegression(C=C, max_iter=1000)
    results += cross_validate(model, f'C={C}', 'Logistic Regression')

# KNN
for k in [3, 5, 7, 9, 11]:
    model = KNeighborsClassifier(n_neighbors=k)
    results += cross_validate(model, f'K={k}', 'KNN')

# SVM
for kernel in ['linear', 'rbf']:
    model = SVC(kernel=kernel)
    results += cross_validate(model, kernel, 'SVM')

# Decision Tree
for depth in [3, 5, 7, None]:
    model = DecisionTreeClassifier(max_depth=depth)
    results += cross_validate(model, str(depth), 'Decision Tree')

# 5. í‰ê·  ê³„ì‚° ë° ìƒìœ„ 5ê°œ ì¶œë ¥
df_results = pd.DataFrame(results)
avg_results = df_results.groupby(['Model', 'Param']).mean(numeric_only=True).reset_index()
top5 = avg_results.sort_values(by='F1 Score', ascending=False).head(5)

# 6. ì‹œê°í™”
fig, ax = plt.subplots(figsize=(10, 2))
ax.xaxis.set_visible(False)
ax.yaxis.set_visible(False)
ax.set_frame_on(False)

tbl = table(ax, top5.round(3), loc='center', colWidths=[0.15]*len(top5.columns))
tbl.auto_set_font_size(False)
tbl.set_fontsize(10)
tbl.scale(1.2, 1.5)

plt.title("Top 5 Model Parameter Combinations (Based on F1 Score)", fontsize=14, weight='bold')
plt.tight_layout()
plt.show()

# 7. ìµœê³ ì˜ ì¡°í•© ì¶œë ¥
best = top5.iloc[0]
print(f"ğŸ† F1 Score ê¸°ì¤€ ìµœê³ ì˜ ì¡°í•©:\nëª¨ë¸: {best['Model']}, íŒŒë¼ë¯¸í„°: {best['Param']}, F1 Score: {best['F1 Score']:.4f}")
